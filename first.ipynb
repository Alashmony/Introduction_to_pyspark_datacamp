{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a3a652b-df91-4aa7-ad9c-078815b384c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pylint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3e391f-a99b-4390-a869-ab04022908d7",
   "metadata": {},
   "source": [
    "## Using DataFrames\n",
    "Spark's core data structure is the Resilient Distributed Dataset (RDD). This is a low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster. However, RDDs are hard to work with directly, so in this course you'll be using the Spark DataFrame abstraction built on top of RDDs.\n",
    "\n",
    "The Spark DataFrame was designed to behave a lot like a SQL table (a table with variables in the columns and observations in the rows). Not only are they easier to understand, DataFrames are also more optimized for complicated operations than RDDs.\n",
    "\n",
    "When you start modifying and combining columns and rows of data, there are many ways to arrive at the same result, but some often take much longer than others. When using RDDs, it's up to the data scientist to figure out the right way to optimize the query, but the DataFrame implementation has much of this optimization built in!\n",
    "\n",
    "To start working with Spark DataFrames, you first have to create a `SparkSession` object from your `SparkContext`. You can think of the `SparkContext` as your connection to the cluster and the `SparkSession` as your interface with that connection.\n",
    "\n",
    "Remember, for the rest of this course you'll have a `SparkSession` called spark available in your workspace!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91c6c624-5b2f-4856-9932-9bf34d461c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e0a0ff-7d40-4427-b920-4c2208c50a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a83bd2a-af77-4b3c-9c27-8c7a85bb89ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://AAsmny-lt-11105.itworx.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f789f35-0616-44be-a88a-ffb23b2cb2e5",
   "metadata": {},
   "source": [
    "## Creating a SparkSession\n",
    "We've already created a SparkSession for you called spark, but what if you're not sure there already is one? Creating multiple SparkSessions and SparkContexts can cause issues, so it's best practice to use the SparkSession.builder.getOrCreate() method. This returns an existing SparkSession if there's already one in the environment, or creates a new one if necessary!\n",
    "\n",
    "* Import SparkSession from pyspark.sql.\n",
    "* Make a new SparkSession called my_spark using SparkSession.builder.getOrCreate().\n",
    "* Print my_spark to the console to verify it's a SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73ea31ab-059e-4551-9f55-5427e28ee414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000020BC237BA30>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create my_spark\n",
    "my_spark = SparkSession(sc).builder.getOrCreate()\n",
    "\n",
    "# Print my_spark\n",
    "print(my_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecca1f2b-7c55-474d-9665-ae71f226e940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://AAsmny-lt-11105.itworx.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x20bc237ba30>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f62de-ae6d-45f0-aa9a-60af0ed3ed4e",
   "metadata": {},
   "source": [
    "## Viewing tables\n",
    "Once you've created a `SparkSession`, you can start poking around to see what data is in your cluster!\n",
    "\n",
    "Your `SparkSession` has an attribute called catalog which lists all the data inside the cluster. This attribute has a few methods for extracting different pieces of information.\n",
    "\n",
    "One of the most useful is the `.listTables()` method, which returns the names of all the tables in your cluster as a list.\n",
    "\n",
    "* See what tables are in your cluster by calling `spark.catalog.listTables()` and printing the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dcab7a-0b10-4889-92a6-4d71441ed458",
   "metadata": {},
   "source": [
    "### Problems running Hadoop on Windows\n",
    "Hadoop requires native libraries on Windows to work properly -that includes to access the file:// filesystem, where Hadoop uses some Windows APIs to implement posix-like file access permissions.\n",
    "\n",
    "This is implemented in HADOOP.DLL and WINUTILS.EXE.\n",
    "\n",
    "In particular, %HADOOP_HOME%\\BIN\\WINUTILS.EXE must be locatable.\n",
    "\n",
    "If it is not, Hadoop or an application built on top of Hadoop will fail.\n",
    "\n",
    "How to fix a missing WINUTILS.EXE\n",
    "You can fix this problem in two ways\n",
    "\n",
    "Install a full native windows Hadoop version. The ASF does not currently (September 2015) release such a version; releases are available externally.\n",
    "Or: get the WINUTILS.EXE binary from a Hadoop redistribution. There is a repository of this for some Hadoop versions on github.\n",
    "Then\n",
    "\n",
    "Set the environment variable %HADOOP_HOME% to point to the directory above the BIN dir containing WINUTILS.EXE.\n",
    "Or: run the Java process with the system property hadoop.home.dir set to the home directory.\n",
    "\n",
    "Refer to: [WindowsProblems](https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems) page on colfluence\n",
    "\n",
    "#### Steps to resolve:\n",
    "* Clone [this repo](https://github.com/steveloughran/winutils) into a folder using:\n",
    "`git clone https://github.com/steveloughran/winutils.git`\n",
    "* Add a new system variable called `HADOOP_HOME` with value of:\n",
    "    `Path you cloned the repo to` +`\\hadoop-3.0.0`\n",
    "   Or depending on the latest version you can find\n",
    "   \n",
    "##### You can try this also:\n",
    "* run: `pip install pyhadoop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a77cb56-7310-4d3d-8816-5602c5b57a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the tables in the catalog\n",
    "print(my_spark.catalog.listTables())\n",
    "my_spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a0a0ca-f0ca-447b-82c1-dd72ad3c29f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
